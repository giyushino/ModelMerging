INFO 07-13 15:57:22 [__init__.py:244] Automatically detected platform cuda.
INFO 07-13 15:57:24 [api_server.py:1395] vLLM API server version 0.9.2
INFO 07-13 15:57:24 [cli_args.py:325] non-default args: {'host': '0.0.0.0', 'port': 55654, 'model': 'Qwen/Qwen2.5-Math-1.5B-Instruct', 'trust_remote_code': True, 'max_model_len': 4096, 'served_model_name': ['Qwen/Qwen2.5-Math-1.5B-Instruct'], 'gpu_memory_utilization': 0.8, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 256, 'disable_log_requests': True}
INFO 07-13 15:57:29 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 07-13 15:57:29 [config.py:1472] Using max model len 4096
INFO 07-13 15:57:29 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 07-13 15:57:33 [__init__.py:244] Automatically detected platform cuda.
INFO 07-13 15:57:35 [core.py:526] Waiting for init message from front-end.
INFO 07-13 15:57:35 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 07-13 15:57:35 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 07-13 15:57:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 07-13 15:57:35 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...
INFO 07-13 15:57:36 [gpu_model_runner.py:1775] Loading model from scratch...
INFO 07-13 15:57:36 [cuda.py:284] Using Flash Attention backend on V1 engine.
INFO 07-13 15:57:36 [weight_utils.py:292] Using model weights format ['*.safetensors']
INFO 07-13 15:57:37 [weight_utils.py:345] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.03it/s]

INFO 07-13 15:57:38 [default_loader.py:272] Loading weights took 1.01 seconds
INFO 07-13 15:57:38 [gpu_model_runner.py:1801] Model loading took 2.8798 GiB and 1.889719 seconds
INFO 07-13 15:57:43 [backends.py:508] Using cache directory: /home/allanz/.cache/vllm/torch_compile_cache/62cbb6517d/rank_0_0/backbone for vLLM's torch.compile
INFO 07-13 15:57:43 [backends.py:519] Dynamo bytecode transform time: 4.58 s
INFO 07-13 15:57:47 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 3.395 s
INFO 07-13 15:57:47 [monitor.py:34] torch.compile takes 4.58 s in total
ERROR 07-13 15:57:48 [core.py:586] EngineCore failed to start.
ERROR 07-13 15:57:48 [core.py:586] Traceback (most recent call last):
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
ERROR 07-13 15:57:48 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 07-13 15:57:48 [core.py:586]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 404, in __init__
ERROR 07-13 15:57:48 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 82, in __init__
ERROR 07-13 15:57:48 [core.py:586]     self._initialize_kv_caches(vllm_config)
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 142, in _initialize_kv_caches
ERROR 07-13 15:57:48 [core.py:586]     available_gpu_memory = self.model_executor.determine_available_memory()
ERROR 07-13 15:57:48 [core.py:586]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 76, in determine_available_memory
ERROR 07-13 15:57:48 [core.py:586]     output = self.collective_rpc("determine_available_memory")
ERROR 07-13 15:57:48 [core.py:586]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
ERROR 07-13 15:57:48 [core.py:586]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 07-13 15:57:48 [core.py:586]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2736, in run_method
ERROR 07-13 15:57:48 [core.py:586]     return func(*args, **kwargs)
ERROR 07-13 15:57:48 [core.py:586]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 07-13 15:57:48 [core.py:586]     return func(*args, **kwargs)
ERROR 07-13 15:57:48 [core.py:586]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 07-13 15:57:48 [core.py:586]   File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 215, in determine_available_memory
ERROR 07-13 15:57:48 [core.py:586]     assert self.init_snapshot.free_memory > free_gpu_memory, (
ERROR 07-13 15:57:48 [core.py:586] AssertionError: Error in memory profiling. Initial free memory 43.37432861328125 GiB, current free memory 43.9400634765625 GiB. This happens when other processes sharing the same container release GPU memory while vLLM is profiling during initialization. To fix this, ensure consistent GPU memory allocation or isolate vLLM in its own container.
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 590, in run_engine_core
    raise e
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 404, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 82, in __init__
    self._initialize_kv_caches(vllm_config)
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 142, in _initialize_kv_caches
    available_gpu_memory = self.model_executor.determine_available_memory()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 76, in determine_available_memory
    output = self.collective_rpc("determine_available_memory")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 57, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/utils/__init__.py", line 2736, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 215, in determine_available_memory
    assert self.init_snapshot.free_memory > free_gpu_memory, (
AssertionError: Error in memory profiling. Initial free memory 43.37432861328125 GiB, current free memory 43.9400634765625 GiB. This happens when other processes sharing the same container release GPU memory while vLLM is profiling during initialization. To fix this, ensure consistent GPU memory allocation or isolate vLLM in its own container.
[rank0]:[W713 15:57:49.239579121 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1495, in <module>
    uvloop.run(run_server(args))
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1431, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1451, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 194, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 162, in from_vllm_config
    return cls(
           ^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 124, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 96, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 666, in __init__
    super().__init__(
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 403, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 434, in launch_core_engines
    wait_for_engine_startup(
  File "/home/allanz/miniconda3/envs/modelmerge/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 484, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
